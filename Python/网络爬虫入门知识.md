# 网络爬虫入门知识


## Requests库

`pip install requests`

可以终端运行如下：

```
$ python
>>> import requests
>>> r = requests.get("https://www.baidu.com")
>>> r.status_code
>>> r.encoding = 'utf-8'
>>> r.text or r.text[:100]
'<!DOCTYPE html>\n<html lang="zh-CN">\n<head>\n\t\n\n\t<meta charset="utf-8">\n\t<meta name="viewport"......
```

requests库的7个主要方法

|方法|说明|
|----|----|
|requests.request()|构造一个请求，支撑以下方法的基础方法|
|requests.get()|----|
|requests.post()|----|
|requests.put()|----|
|requests.patch()|----|
|requests.delete()|----|

```
requests.get(url, params=None, **kwargs)

url: 拟获取页面的url链接
params: url中的额外参数，字典或字节流格式
**kwargs: 12个控制访问参数
```

### Requests库的异常
|异常|说明|
|----|----|
|requests.ConnectionError | 网络连接错误异常，如DNS查询失败、拒绝连接等 |
| requests.HTTPError | HTTP错误异常 |
|requests. URLRequired | URI缺失异常 |
|requests.TooManyRedirects |超过最大重定向次数，产生重定向异常|
|requests.ConnectTimeout | 连接远程服务器超时异常 |
|requests.Timeout | 请求URL超时，产生超时异常 |

### Response对象的属性

Response 包含 请求的所有返回数据内容

|属性|说明|
|----|----|
|r.status_code|HTTP请求的返回状态，200表示连接成功，404表示失败|
|r.text|HTTP响应内容的字符串形式，即，ur1对应的页面内容|
|r.encoding|从HTTP header中猜测的响应内容编码方式|
|r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）|
|r.content|HTTP响应内容的二进制形式|

> `r.encoding`: 如果header中不存在charset, 则认为编码为ISO-8859-1
> `r.apparent_encoding`: 根据网页内容分析出的编码方式

### Response异常

`r.raise_for_status()`: 如果状态不是200， 引发HTTPError异常

## 爬虫的规模

* 小规模，数据量小，爬取的速度不敏感 - `Request库`
* 中规模，数据规模较大，爬取速度敏感 - `Scrapy库`
* 大规模， 搜索引擎，爬取速度关键 - 定制开发


## Robots协议
`Robots Exclusion Standard` 网络爬虫拜除标准，以'robots.txt'命名放在网站的根目录下，如

> 
- `https://www.baidu.com/robots.txt`
- `http://news.sina.com.cn/robots.txt`

- 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。
- 形式：在网站根目录下的robots.txt文件。

#### Robots协议基本语法
```
# 注释 *代表所有 /代表根目录
User-agent: *
Disallow: /
```

> 如果一个网站没有robots协议文件，表示允许所的人无限制的爬取其内容

#### Robots协议的使用
- 网络爬虫：白动或人工识别robots.txt，再进行内
容爬取。
- 约束性：Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。

爬取网页 玩转网页，坊问量很小：可以遵宁：
非商业且偶尔，访问量较大：建议遵守；
商业利益或爬取全网：必须遵守。


## Beautiful Soup
Beautiful Soup 是可以用来解析，遍历，维护‘标签树’的功能库。

安装：
`pip install beautifulsoup4`

将网页变成`Beautiful Soup`

```
from bs4 import BeautifulSoup
soup = BeautifulSoup(r.content, "html.parser")
print(soup.prettify())
```

> 更多关于‘标签树’的知识：
> 
 [Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html) and 
 [XML 的 XPath 语法](https://blog.walterlv.com/post/xml-xpath.html) and 
 [XPath type system](https://www.ibm.com/docs/en/i/7.1?topic=xpath-type-system) and 
 [Java XPath 示例 – XPath 教程](https://github.com/apachecn/howtodoinjava-zh/blob/master/docs/misc2/59.md)
 
#### Beautiful Soup

`html` <---> 标签树 <---> `Beautiful Soup`类
```
from bs4 import BeautifulSoup
soup = BeautifulSoup("<html>data</html>", "html.parser")
print(soup.prettify(open("D://demo.html"), "html.parser"))
```

Beautiful Soup 对应一个html/xml文档的所有内容

#### Beautiful Soup库解析器
|解析器|使用方法|条件|
|----|----|----|
| bs4的HTML解析器 | BeautifulSoup(mk,'html.parser') | 安装bs4库  |
| lxml的HTML解析器 | BeautifulSoup(mk,'lxml') | `pip install lxml` |
| lxml的xML解析器 | BeautifulSoup(mk,'xml') | `pip install lxml` |
| html5lib的解析器 | BeautifulSoup(mk,'html5lib") | `pip install html5lib` |

#### Beautiful Soup的基本元素

|基本元素|说明|
|----|----|
|Tag| 标签，最基本的信息组织单元，分别用`<>`和`</>`标明开头和结尾 |
|Name| 标签的名字，`<p>…</p>`的名字是'p'，格式：`<tag>.name` |
|Attributes| 标签的属性，字典形式组织，格式：`<tag>.attrs` |
|NavigableString| 标签内非属性字符串，`<>…</>`中字符串，格式：`<tag>.string` |
|Comment| 标签内宇符串的注释部分，一种特殊的Comment类型 |


### Beautiful Soup标签树遍历

#### 标签树的下行遍历
|属性|说明|
|----|----|
| .contents | 子节点的列表，将`<tag>`所有儿子节点存人列表 |
| .children | 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点 |
| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历 |

#### 标签树的上行遍历
|属性|说明|
|----|----|
| .parent | 节点的父亲标签 |
| .parents | 节点先辈标签的迭代类型，用于循环遍历先辈节点 |

#### 标签树的平行遍历
|属性|说明|
|----|----|
| .next_sibling | 返回按照HTML文本顺序的下一个平行节点标签 |
| .previous_sibling | 返回按照HTML文本顺序的上一个平行节点标签 |
| .next_siblings | 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 |
| .previous_siblings | 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 |

示例：

```
# 遍历后续节点
for sibling in soup.a.next_siblings:
	print(sibling)
	
# 遍历前续节点
for sibling in soup.a.previous_siblings:
	print (sibling)
```

```
<note>
<to>Tove</to>
<from>Jani</from>
<heading>Reminder</heading>
<body>Don't forget me this weekend!</body>
</note>
```

## 信息组织和提取方法

### 信息标记三种形式

- `XML` 最早的通用信息标记语言，可扩展性好，但繁琐。Internet 上的信息交互与传递。
- `JSON` 信息有类型，适合程序处理(js)，较XML简洁。移动应用云端和节点的信息通信，无注释。
- `YAML` 信息无类型，文本信息比例最高，可读性好。各类系统的配置文件，有注释易读。

XML示例：

```
<?xml version="1.0" encoding="utf-8"?>
<note> 
  <date> 
    <day>10</day>  
    <month>01</month>  
    <year>2008</year> 
  </date>  
  <to>Tove</to>  
  <from>Jani</from>  
  <heading>Reminder</heading>  
  <body>Don't forget me this weekend!</body> 
</note>
```

`JSON`示例：

```
{
    "id": 1,
    "name": "Java核心技术",
    "author": {
        "firstName": "Abc",
        "lastName": "Xyz"
    },
    "isbn": "1234567",
    "tags": ["Java", "Network"]
}
```

`YAML`示例：

```
languages:
  - Ruby
  - Perl
  - Python 
websites:
  YAML: yaml.org 
  Ruby: ruby-lang.org 
  Python: python.org 
  Perl: use.perl.org
```

### 信息提取的一般方法

- 方法一：完整解析信息的标记形式，再提取关键信息。

JSON XML YAML 文件
>
需要标记解析器例如：bs4库的标签树遍历
>
>优点：信息解析准确
>
>缺点：提取过程繁琐，速度慢。


- 方法二：无视标记形式，直接搜索关键信息。

搜索
> 对信息的文本查找函数即可。

> 优点：提取过程简洁，速度较快。

> 缺点：提取结果准确性与信息内容相关。

- 方法二 融合方法：结合形式解析与搜索方法，提取关键信息。

>
XML JSON YAIVL 搜索
>
需要标记解析器及文本查找函数。

#### 实例: 提取HTML中所有URL链接

>
思路：
>
1）搜索到所有`<a>`标签
>
2）解析`<a>`标签格式，提取`href`后的链接内容。


### 基于bs4库的HITML内容查找方法

`find _all` 方法：返回一个列表类型，存储查找的结果。

`<>.find _all(name, attrs, recursive, string, **kwargs)`

- `name`: 对标签名称的检索字符串。
- `attrs`: 对标签属性值的检索字符串，可标注属性检索。 
- `recursive`：是否对子孙全部检索，默认True
- `string`: `<>…</>`中字符串区域的检索宇符串。

#### 示例：

#### 参数`name`示例：

* 查找并打印所有子孙节点内`a`标签的`href`值: 

```
for link in soup.find all ('a'):
	print (link.get('href'))
```

* 查找获取多个标签，同时获取`a`及`p`标签：

`soup.find_all(['a', 'p'])`

* 当name的值为true的时候，将获取节点内所有的标签：

```
for tag in soup.find_all (True)
	print (tag.name)
```

* 使用正则进行匹配，匹配所有以`b`开头的标签：

```
import re
for tag in soup.find_all(re.compile ('b')):
	print (tag.name)
```


`soup.find_all('p', 'course')`: 查找所有包含`course`的`p`标签
`soup.find_all(id='link')`: 查找所有属性`id`严格为`link`的标签，如`id`为`link1`将不会被匹配

使用正则匹配`id`属性为`link`开头的标签：

```
import re
soup.find_all(id=re.compile('link'))
```

`soup.find all('a', recursive=False)`: 查找仅当前节点（不包含子节点）的a标签

##### 按`<>…</>`中的字符串进行检索：

`soup.find_all(string = "Basic Python")`: 查找所有text中包含`Basic Python`的标签

或使用正则进行匹配：
```
import re
soup.find_all(string = re.compile("python"))
```

#### find_all()的扩展方法

|方法|说明|
|----|----|
| <>.find() | 搜索且只返回一个结果，字符串类型，同.find_all()参数 |
| <>.find_parents() | 在先辈节点中搜索，返回列表类型，同.find_all()参数 |
| <>.find_parent() | 在先辈节点中返回一个结果，字符串类型，同.find()参数 |
| <>.find_next_siblings() | 在后续平行节点中搜索，返回列表类型，同.find_all()参数 |
| <>.find_next_sibling() | 在后续平行节点中返回一个结果，字符串类型，同.find()参数 |
| <>.fiind_previous_siblings() | 在前序平行节点中搜索，返回列表类型，同.find_all()參数 |
| <>.find_previous_sibling() | 在前序平行节点中返回一个结果，字符串类型，同.find()参数 |
